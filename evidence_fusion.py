import numpy as np
from typing import List, Tuple, Dict, Optional
import json
from datetime import datetime
import pandas as pd
from collections import defaultdict
import itertools
import os
from pathlib import Path

OUTPUT_DIR = os.path.expanduser("~/Desktop/Degree Project/2DV50E/outputs")
os.makedirs(OUTPUT_DIR, exist_ok=True)

# ==============================================================================
# Part 1: Core Evidence Fusion System
# ==============================================================================

class EvidenceFusion:
    
    def __init__(self):
        # Feature weight vector (8 features)
        # [F1:Response Status Code, F2:Response Time, F3:Content Difference, F4:Vulnerability Type, 
        # F5:Payload Success Rate, F6:Database Fingerprint, F7:Data Extraction, F8:Sensitive Fields]
        self.feature_weights = np.array([0.10, 0.15, 0.20, 0.25, 0.15, 0.08, 0.05, 0.02])
        
        # Tool weights (based on historical accuracy)
        self.tool_weights = {
            'sqlmap': 0.60,  # SQLMap weights
            'zap': 0.40      # OWASP ZAP weights
        }
        
        # Heuristic rule parameters
        self.consistency_threshold = 0.15  # Consistency Determination Threshold
        self.divergence_threshold = 0.30   # Reference threshold for non-linear divergence penalty
        self.consistency_bonus = 0.10      # Consistency Bonus Coefficient (10%)
        self.divergence_penalty_base = 0.15  # Base Divergence Penalty Coefficient (15%)
        self.divergence_penalty_max = 0.25   # Maximum Divergence Penalty (25%)
        self.strong_evidence_threshold = 0.8  # Strong Evidence Threshold
        self.strong_evidence_floor = 0.80  # Lower Bound of Evidence Credibility
        self.medical_risk_bonus = 0.55     # Medical Risk Multiplicative Bonus (55%, increased to fix FN cases)
        
        # Adaptive thresholds for medical scenarios
        # This is a core innovation of my thesis!
        self.medical_thresholds = {
            'L1': { # Critical Risk Module (Electronic Prescriptions, Surgical Records)
                'high': 0.11,      # Lower threshold to increase FPR (allow false positives)
                'medium': 0.30,    # Moderate Risk Threshold
                'sla_high': '6h',  # High-Risk Response Time
                'sla_medium': '24h',
                'description': 'Fatal Risk - Directly Threatening the Patient Life'
            },
            'L2': { # High Risk Module (Patient Information, Medical Records, Authentication)
                'high': 0.48,      # Lower threshold to increase Recall
                'medium': 0.35,
                'sla_high': '12h',
                'sla_medium': '48h',
                'description': 'High Risk - Massive Privacy Breach'
            },
            'L3': { # Medium Risk Modules (Inspection Reports, Medical Insurance Settlement)
                'high': 0.55,      # Higher threshold to reduce FPR (proven effective)
                'medium': 0.40,
                'sla_high': '24h',
                'sla_medium': '1week',
                'description': 'Medium Risk'
            },
            'L4': { # Low Risk Modules (Registration, Scheduling, Inventory)
                'high': 0.52,      # Slightly higher threshold to filter TC034 (causes 20% Recall drop)
                'medium': 0.45,
                'sla_high': '3days',
                'sla_medium': '2weeks',
                'description': 'Low Risk'
            }
        }
        
        # Standard thresholds (non-medical scenarios)
        self.standard_thresholds = {
            'high': 0.80,
            'medium': 0.50,
            'low': 0.30
        }
        
        # Medical data sensitivity classification
        # Core Innovation: Automatically identifies sensitive medical data and escalates risk levels.
        self.data_sensitivity_keywords = {
            'CRITICAL': { # Extrememly Sensitive Data
                'keywords': ['prescription', 'drug', 'medication', 'hiv', 'aids', 
                        'psychiatric', 'mental', 'genetic', 'dna'],
                'description': 'Prescription information, HIV status, mental health diagnoses, genetic information',
                'consequence': 'Life-threatening risks and severe discrimination risks'
            },
            'HIGH': { # Highly Sensitive Data
                'keywords': ['diagnosis', 'surgery', 'operation', 'pathology', 
                        'treatment', 'therapy', 'oncology', 'cancer'],
                'description': 'Diagnostic findings, surgical records, pathology reports',
                'consequence': 'Severe privacy violations'
            },
            'MEDIUM': { # Sensitive Data
                'keywords': ['patient', 'medical', 'health', 'record', 'history',
                        'visit', 'admission'],
                'description': 'Patient Basic Information, Medical Records',
                'consequence': 'General privacy breaches'
            },
            'LOW': { # General Data
                'keywords': ['appointment', 'schedule', 'department', 'doctor',
                        'nurse', 'room', 'bed'],
                'description': 'Registration Information, Department Information',
                'consequence': 'Business Information Leakage'
            }
        }
        
        # Ablation study control flags (for disabling rules during experiments)
        self._disable_rule2 = False  # Control flag for Rule 2 (Non-linear Divergence Penalty)
        self._disable_rule4 = False  # Control flag for Rule 4 (Medical Risk Multiplicative Bonus)
    
    # ========================================================================
    # Evidence Collection and Fusion 
    # ========================================================================
    
    def extract_features_from_sqlmap(self, sqlmap_result: Dict) -> np.ndarray:
        features = np.zeros(8)
        
        # F1: Response status codes
        status_code = sqlmap_result.get('status_code', 200)
        features[0] = 0.5 if status_code in [500, 503] else 0.0
        
        # F2: Response Time
        delay_time = sqlmap_result.get('delay_time', 0)
        features[1] = min(delay_time / 10.0, 1.0)
        
        # F3: Content Diversity
        error_keywords = sqlmap_result.get('error_keywords', [])
        features[2] = min(len(error_keywords) / 10.0, 1.0)
        
        # F4: Vulnerability Severity Level
        vuln_type = sqlmap_result.get('vulnerability_type', '')
        vuln_severity = {
            'UNION query': 0.95,
            'Error-based': 0.90,
            'Stacked queries': 0.85,
            'Stacked Query': 0.85,
            'Boolean-based blind': 0.80,
            'Boolean-based Blind': 0.80,
            'Time-based blind': 0.70,
            'Time-based Blind': 0.70,
            'Time-based': 0.70,
            # POST/Header/Cookie variants (common in medical web apps)
            'Error-based (POST)': 0.88,
            'Error-based (POST Double Quote)': 0.88,
            'Error-based (Password Update)': 0.87,
            'Error-based (Header Injection)': 0.86,
            'Error-based (Referer Injection)': 0.86,
            'Error-based (Cookie Injection)': 0.85,
            'Error-based (Comments)': 0.88,
            'Stacked Query (POST)': 0.83,
            'Outfile Injection': 0.82,
            # Additional variants
            'Error-based (Single Quote)': 0.90,
            'Error-based (Numeric)': 0.90,
            'Error-based (Single Quote with Parenthesis)': 0.89,
            'Error-based (Double Quote)': 0.89,
        }
        features[3] = vuln_severity.get(vuln_type, 0.0)
        
        # F5: Payload Success Rate
        successful_payloads = sqlmap_result.get('successful_payloads', 0)
        total_payloads = sqlmap_result.get('total_payloads', 1)
        features[4] = successful_payloads / max(total_payloads, 1)
        
        # F6: Database Fingerprinting
        features[5] = 1.0 if sqlmap_result.get('dbms_detected', False) else 0.0
        
        # F7: Data extraction successful
        features[6] = 1.0 if sqlmap_result.get('data_dumped', False) else 0.0
        
        # F8: Medical Sensitive Field Detection
        param_name = sqlmap_result.get('parameter_name', '').lower()
        healthcare_keywords = ['patient', 'prescription', 'diagnosis', 'medical', 'ssn', 'health']
        features[7] = 1.0 if any(kw in param_name for kw in healthcare_keywords) else 0.0
        
        return features
    
    def extract_features_from_zap(self, zap_result: Dict) -> np.ndarray:
        features = np.zeros(8)
        
        # F1: Response Status Code
        status_code = zap_result.get('status_code', 200)
        features[0] = 0.5 if status_code in [500, 503] else 0.0
        
        # F2: Response Time
        response_time_ms = zap_result.get('response_time', 0)
        features[1] = min(response_time_ms / 10000.0, 1.0)
        
        # F3: Page Length Change Rate
        length_change_rate = zap_result.get('content_length_change', 0)
        features[2] = min(abs(length_change_rate), 1.0)
        
        # F4: Alert Level
        alert_level = zap_result.get('alert_level', 'Low')
        alert_severity = {
            'High': 0.90,
            'Medium': 0.70,
            'Low': 0.40,
            'Informational': 0.20
        }
        features[3] = alert_severity.get(alert_level, 0.0)
        
        # F5: Trigger Rule Ratio
        triggered_rules = zap_result.get('triggered_rules', 0)
        total_rules = zap_result.get('total_rules', 1)
        features[4] = triggered_rules / max(total_rules, 1)
        
        # F6: Database Fingerprinting
        features[5] = 1.0 if zap_result.get('dbms_detected', False) else 0.0
        
        # F7: ZAP does not support data extraction
        features[6] = 0.0
        
        # F8: Medical Sensitive Field Detection
        param_name = zap_result.get('parameter_name', '').lower()
        healthcare_keywords = ['patient', 'prescription', 'diagnosis', 'medical', 'ssn', 'health']
        features[7] = 1.0 if any(kw in param_name for kw in healthcare_keywords) else 0.0
        
        return features
    
    def calculate_single_tool_confidence(self, features: np.ndarray, 
                                        is_zap: bool = False) -> float:
        weights = self.feature_weights.copy()
        
        # If ZAP, F7 weight is 0, need to renormalize
        if is_zap:
            weights[6] = 0.0
            weights = weights / weights.sum()
        
        # Weighted sum
        confidence = np.dot(weights, features)
        return float(confidence)
    
    def weighted_fusion(self, s_sqlmap: float, s_zap: float) -> float:
        c_base = (self.tool_weights['sqlmap'] * s_sqlmap + 
                  self.tool_weights['zap'] * s_zap)
        return c_base
    
    
    def apply_heuristic_rules(self, c_base: float, s_sqlmap: float, s_zap: float,
                            features_sqlmap: np.ndarray, 
                            features_zap: np.ndarray,
                            module_risk_level: Optional[str] = None) -> Tuple[float, List[str]]:
        c_adjusted = c_base
        triggered_rules = []
        diff = abs(s_sqlmap - s_zap)
        
        # Rule 1: Consistency Rewards
        # When the results from two tools show high consistency, confidence is enhanced.
        if diff < self.consistency_threshold:
            c_adjusted *= (1 + self.consistency_bonus)
            triggered_rules.append(f"Rule 1 - Consistency Reward: diff={diff:.3f} < {self.consistency_threshold}, "
                                f"Confidence level √ó {1+self.consistency_bonus}")
        
        # Rule 2: Strong Evidence Boost
        # When explicit SQL error messages are detected, this constitutes strong evidence.
        if (features_sqlmap[2] > self.strong_evidence_threshold or 
            features_zap[2] > self.strong_evidence_threshold):
            old_c = c_adjusted
            c_adjusted = max(c_adjusted, self.strong_evidence_floor)
            if c_adjusted > old_c:
                triggered_rules.append(f"Rule 2 - Strong Evidence Boost: F3={max(features_sqlmap[2], features_zap[2]):.3f} > "
                                    f"{self.strong_evidence_threshold}, Confidence level increased to ‚â• {self.strong_evidence_floor}")
        
        # Rule 3: Medical Risk-Based Multiplicative Weighting
        # L1/L2 modules receive multiplicative bonus based on risk severity
        if not getattr(self, '_disable_rule3', False):  # Changed from _disable_rule4
            if module_risk_level in ['L1', 'L2']:
                old_c = c_adjusted
                c_adjusted *= (1 + self.medical_risk_bonus)
                c_adjusted = min(c_adjusted, 1.0)  # Cap at 1.0
                triggered_rules.append(f"Rule 3 - Medical Risk Multiplicative Bonus: {module_risk_level}, "
                                    f"Confidence {old_c:.4f} √ó {1+self.medical_risk_bonus} = {c_adjusted:.4f}")
        
        # Ensure that the confidence level falls within the range [0, 1].
        c_adjusted = max(0.0, min(1.0, c_adjusted))
        return c_adjusted, triggered_rules
    
    # ========================================================================
    # Confidence-Based Stratification Mechanism
    # ========================================================================
    
    def get_adaptive_thresholds(self, module_risk_level: Optional[str] = None) -> Dict:
        if module_risk_level and module_risk_level in self.medical_thresholds:
            return self.medical_thresholds[module_risk_level]
        else:
            return {
                'high': self.standard_thresholds['high'],
                'medium': self.standard_thresholds['medium'],
                'sla_high': '1week',
                'sla_medium': '2weeks',
                'description': 'Standard Risk Level'
            }
    
    def classify_confidence(self, c_final: float, thresholds: Dict) -> Dict:
        if c_final >= thresholds['high']:
            return {
                'risk_level': 'HIGH',
                'confidence_level': 'High Confidence',
                'description': 'Confirmed SQL injection vulnerability',
                'action': 'IMMEDIATE_REVIEW',
                'priority': 'CRITICAL',
                'sla': thresholds.get('sla_high', '1week')
            }
        elif c_final >= thresholds['medium']:
            return {
                'risk_level': 'MEDIUM',
                'confidence_level': 'Medium Confidence',
                'description': 'Potential SQL injection vulnerability',
                'action': 'SCHEDULED_REVIEW',
                'priority': 'HIGH',
                'sla': thresholds.get('sla_medium', '2weeks')
            }
        elif c_final >= 0.30:
            return {
                'risk_level': 'LOW',
                'confidence_level': 'Low confidence',
                'description': 'SQL injection vulnerabilities may exist.',
                'action': 'LOG_AND_MONITOR',
                'priority': 'LOW',
                'sla': 'Regular review'
            }
        else:
            return {
                'risk_level': 'VERY_LOW',
                'confidence_level': 'Extremely low confidence',
                'description': 'Basic Safety',
                'action': 'NO_ACTION',
                'priority': 'INFO',
                'sla': 'No need to pay attention'
            }
    
    # ========================================================================
    # Decision Rule Engine (Remove)!!!
    # ========================================================================
    
    def make_decision(self, c_final: float, module_info: Dict) -> Dict:
        # Step 1: Obtain adaptive threshold
        module_risk_level = module_info.get('risk_level', None)
        thresholds = self.get_adaptive_thresholds(module_risk_level)
        
        # Step 2: Hierarchical Decision Making Based on Confidence Levels
        classification = self.classify_confidence(c_final, thresholds)
        
        # Step 3: Generate Response Policy
        decision = {
            # Basic Information
            'target': module_info.get('target_url', 'N/A'),
            'module_name': module_info.get('name', 'Unknown Module'),
            'module_risk_level': module_risk_level or 'N/A',
            'parameter_name': module_info.get('parameter_name', 'N/A'),
            
            # Confidence Information
            'final_confidence': c_final,
            'confidence_level': classification['confidence_level'],
            
            # Decision-making information
            'risk_level': classification['risk_level'],
            'action': classification['action'],
            'priority': classification['priority'],
            'description': classification['description'],
            
            # Response Strategy
            'response_sla': classification['sla'],
            'notification_channels': self._get_notification_channels(classification['risk_level']),
            'escalation_path': self._get_escalation_path(classification['risk_level']),
            
            # Detailed Recommendations
            'recommendation': self._generate_recommendation(
                classification['risk_level'], 
                module_info, 
                c_final
            ),
            
            # Timestamp
            'timestamp': datetime.now().isoformat()
        }
        
        return decision
    
    def _get_notification_channels(self, risk_level: str) -> List[str]:
        if risk_level == 'HIGH':
            return ['SMS', 'Email', 'Ticketing System', 'Slack']
        elif risk_level == 'MEDIUM':
            return ['Email', 'Ticketing System']
        elif risk_level == 'LOW':
            return ['Log only']
        else:
            return []
    
    def _get_escalation_path(self, risk_level: str) -> List[str]:
        if risk_level == 'HIGH':
            return ['Security Team Lead', 'CISO', 'CTO']
        elif risk_level == 'MEDIUM':
            return ['Security Team Lead']
        else:
            return []
    
    def _generate_recommendation(self, risk_level: str, module_info: Dict, 
                                confidence: float) -> str:
        module_name = module_info.get('name', 'Target System')
        param_name = module_info.get('parameter_name', 'Unknown parameter')
        
        if risk_level == 'HIGH':
            return (f"Detect {module_name} existed High-confidence SQL injection vulnerabilityÔºàConfidence LevelÔºö{confidence:.2f}ÔºâÔºå"
                f"parameter {param_name} Not sufficiently filtered. Manual verification is recommended immediately. Once confirmed, urgent remediation is required.")
        elif risk_level == 'MEDIUM':
            return (f"{module_name} parameter: {param_name}Potential SQL injection vulnerabilityÔºàConfidence LevelÔºö{confidence:.2f}ÔºâÔºå"
                f"It is recommended to arrange for a safety engineer to conduct a manual review. ")
        elif risk_level == 'LOW':
            return (f"{module_name} Detected potential risks with low confidenceÔºàConfidence LevelÔºö{confidence:.2f}ÔºâÔºå"
                f"This may be a false positive. It is recommended to log the incident and review it during regular security assessments.")
        else:
            return f"{module_name} No significant SQL injection risks were detected."
    
    # ========================================================================
    # Medical Special Rules
    # ========================================================================
    
    def detect_data_sensitivity(self, param_name: str) -> Tuple[str, Dict]:
        param_lower = param_name.lower()
        
        # Check by Priority (Highest to Lowest)
        for level in ['CRITICAL', 'HIGH', 'MEDIUM', 'LOW']:
            keywords = self.data_sensitivity_keywords[level]['keywords']
            if any(kw in param_lower for kw in keywords):
                return level, self.data_sensitivity_keywords[level]
        return 'LOW', self.data_sensitivity_keywords['LOW']
    
    def apply_medical_rules(self, decision: Dict, module_info: Dict) -> Dict:
        medical_adjustments = []
        
        # ==== Rule 1: Data Sensitivity Identification ====
        param_name = module_info.get('parameter_name', '')
        sensitivity_level, sensitivity_info = self.detect_data_sensitivity(param_name)
        decision['data_sensitivity'] = {
            'level': sensitivity_level,
            'description': sensitivity_info['description'],
            'consequence': sensitivity_info['consequence']
        }
        
        # Risk escalation: CRITICAL data's MEDIUM risk ‚Üí HIGH risk
        if sensitivity_level == 'CRITICAL' and decision['risk_level'] == 'MEDIUM':
            original_level = decision['risk_level']
            decision['risk_level'] = 'HIGH'
            decision['priority'] = 'CRITICAL'
            decision['action'] = 'IMMEDIATE_REVIEW'
            decision['response_sla'] = '6h'
            medical_adjustments.append(
                f"Enhanced Data Sensitivity: {original_level} ‚Üí HIGH (Extremely sensitive data detected: {sensitivity_info['description']})"
            )
        
        # ===== Rule 2: Business Continuity Considerations =====
        module_risk = module_info.get('risk_level', 'L4')
        if module_risk in ['L1', 'L2']:
            decision['remediation_plan'] = {
                'window': 'OFF_PEAK_HOURS', 
                'require_backup': True,
                'require_rollback_plan': True,
                'notification': 'Advance notice to clinical staff required'
            }
            medical_adjustments.append(
                f"Business Continuity Rules: {module_risk}The module must be repaired during off-peak hours, and a backup plan must be prepared."
            )
        else:
            decision['remediation_plan'] = {
                'window': 'FLEXIBLE',
                'require_backup': False,
                'require_rollback_plan': False,
                'notification': 'Standard notification'
            }
        
        # ===== Rule 3: HIPAA Compliance Marking =====
        decision['hipaa_compliance'] = {
            'breach_notification_required': decision['risk_level'] == 'HIGH',
            'risk_assessment_required': True,
            'audit_trail_required': True,  
            'corrective_action_required': decision['risk_level'] in ['HIGH', 'MEDIUM'],
            'compliance_note': self._get_hipaa_compliance_note(decision['risk_level'])
        }
        
        if decision['risk_level'] == 'HIGH':
            medical_adjustments.append(
                "HIPAA Compliance: Triggering the Breach Notification Assessment Process (¬ß164.308(a)(6))"
            )
        
        # Record all medical strategy adjustments
        decision['medical_adjustments'] = medical_adjustments
        return decision
    
    def _get_hipaa_compliance_note(self, risk_level: str) -> str:
        
        if risk_level == 'HIGH':
            return ("Notify affected individuals and HHS within 60 days.")
        elif risk_level == 'MEDIUM':
            return ("The complete risk assessment process and corrective actions must be documented to meet HIPAA periodic review requirements.")
        else:
            return ("Audit trail records must be retained for compliance review.")
    
    # ========================================================================
    # Main Fusion Function - Integrates All Features
    # ========================================================================
    
    def fuse(self, sqlmap_result: Dict, zap_result: Dict, 
            module_info: Optional[Dict] = None, verbose: bool = False) -> Dict:

        if module_info is None:
            module_info = {
                'name': 'Unknown Module',
                'risk_level': None,
                'parameter_name': sqlmap_result.get('parameter_name', 'unknown'),
                'target_url': 'N/A'
            }
        
        # ========== Step 1: Feature Extraction ==========
        features_sqlmap = self.extract_features_from_sqlmap(sqlmap_result)
        features_zap = self.extract_features_from_zap(zap_result)
        
        if verbose:
            print("=" * 70)
            print("Step 1: Feature Extraction")
            print("-" * 70)
            print(f"SQLMap features: {features_sqlmap}")
            print(f"ZAP features:    {features_zap}")
        
        # ========== Step 2: Calculate Single-Tool Confidence ==========
        s_sqlmap = self.calculate_single_tool_confidence(features_sqlmap, is_zap=False)
        s_zap = self.calculate_single_tool_confidence(features_zap, is_zap=True)
        
        if verbose:
            print("\n" + "=" * 70)
            print("Step 2: Calculate Single-Tool Confidence")
            print("-" * 70)
            print(f"SQLMap Confidence: {s_sqlmap:.4f}")
            print(f"ZAP Confidence:    {s_zap:.4f}")
            print(f"Difference: {abs(s_sqlmap - s_zap):.4f}")
        
        # ========== Step 3: Basic Weighted Fusion ==========
        c_base = self.weighted_fusion(s_sqlmap, s_zap)
        
        if verbose:
            print("\n" + "=" * 70)
            print("Step 3: Basic Weighted Fusion")
            print("-" * 70)
            print(f"C_base = {self.tool_weights['sqlmap']} √ó {s_sqlmap:.4f} + "
                f"{self.tool_weights['zap']} √ó {s_zap:.4f}")
            print(f"C_base = {c_base:.4f}")
        
        # ========== Step 4: Apply heuristic rules ==========
        c_final, triggered_rules = self.apply_heuristic_rules(
            c_base, s_sqlmap, s_zap, features_sqlmap, features_zap,
            module_risk_level = module_info.get('risk_level')
        )
        
        if verbose:
            print("\n" + "=" * 70)
            print("Step 4: Apply heuristic rules")
            print("-" * 70)
            if triggered_rules:
                for rule in triggered_rules:
                    print(f"‚úì {rule}")
            else:
                print("No rules triggered")
            print(f"\nFinal Confidence Level: {c_final:.4f}")
        
        # ========== Step 5: Layered Decision-Making==========
        decision = self.make_decision(c_final, module_info)
        
        if verbose:
            print("\n" + "=" * 70)
            print("Step 5: Layered Decision-Making")
            print("-" * 70)
            print(f"Module Name: {decision['module_name']}")
            print(f"Risk Level: {decision['module_risk_level']}")
            print(f"Confidence Level: {decision['confidence_level']} ({c_final:.4f})")
            print(f"Risk Assessment: {decision['risk_level']}")
            print(f"Processing Action: {decision['action']}")
            print(f"Priority: {decision['priority']}")
            print(f"Respond to SLA: {decision['response_sla']}")
            print(f"Notification Method: {', '.join(decision['notification_channels'])}")
            if decision['escalation_path']:
                print(f"Upgrade Path: {' ‚Üí '.join(decision['escalation_path'])}")
        
        # ========== Step 6: Special Medical Rules ==========
        decision = self.apply_medical_rules(decision, module_info)
        
        if verbose:
            print("\n" + "=" * 70)
            print("Step 6: Special Medical Rules")
            print("-" * 70)
            print(f"Data Sensitivity: {decision['data_sensitivity']['level']} - "
                f"{decision['data_sensitivity']['description']}")
            print(f"Repair Window: {decision['remediation_plan']['window']}")
            print(f"Backup solution required: {decision['remediation_plan']['require_backup']}")
            print(f"HIPAA Breach Notification: {decision['hipaa_compliance']['breach_notification_required']}")
            
            if decision['medical_adjustments']:
                print("\nAdjustments to Medical Regulations:")
                for adj in decision['medical_adjustments']:
                    print(f"{adj}")
        
        # ========== Final Output ==========
        if verbose:
            print("\n" + "=" * 70)
            print("Final Decision Recommendation")
            print("-" * 70)
            print(decision['recommendation'])
            print("=" * 70)
        
        # Add evidence information
        decision['evidence'] = {
            'sqlmap_confidence': s_sqlmap,
            'zap_confidence': s_zap,
            'base_confidence': c_base,
            'confidence_diff': abs(s_sqlmap - s_zap),
            'triggered_rules': triggered_rules,
            'features_sqlmap': features_sqlmap.tolist(),
            'features_zap': features_zap.tolist()
        }
        
        return decision


# ==============================================================================
# Part 2: Experimental Evaluation Framework
# ==============================================================================

class ExperimentEvaluator:
    
    def __init__(self, fusion_system: EvidenceFusion):
        self.fusion = fusion_system
        self.original_params = {
            'tool_weights': self.fusion.tool_weights.copy(),
            'feature_weights': self.fusion.feature_weights.copy(),
            'consistency_threshold': self.fusion.consistency_threshold,
            'divergence_threshold': self.fusion.divergence_threshold,
            'medical_risk_bonus': self.fusion.medical_risk_bonus
        }
    
    # ========================================================================
    # Test Case Management
    # ========================================================================
    
    def load_test_cases(self, json_file: str) -> List[Dict]:
        try:
            with open(json_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            if 'test_cases' not in data:
                raise ValueError("The JSON file must contain the ‚Äòtest_cases‚Äô field.")
            
            test_cases = data['test_cases']
            print(f"Loaded successfully {len(test_cases)} test case") #Remove!!!
            return test_cases
            
        except FileNotFoundError:
            raise FileNotFoundError(f"The test case file does not exist: {json_file}")
        except json.JSONDecodeError as e:
            raise ValueError(f"JSON format error: {e}")
    
    # ========================================================================
    # Batch Detection and Performance Evaluation
    # ========================================================================
    
    def run_batch_detection(self, test_cases: List[Dict], 
                        method: str = 'full') -> List[Dict]:
        results = []
        
        print(f"Start batch detection (Method: {method})...") #Remove!!!
        print(f"Total {len(test_cases)} test case")
        
        for i, tc in enumerate(test_cases, 1):
            # Module Information
            module_info = {
                'name': tc['medical_context']['module'],
                'risk_level': tc['medical_context']['risk_level'],
                'parameter_name': tc['medical_context']['parameter'],
                'target_url': tc.get('url', 'N/A')
            }
            
            # Select the detection method based on the method
            if method == 'full':
                decision = self.fusion.fuse(
                    tc['sqlmap_result'],
                    tc['zap_result'],
                    module_info,
                    verbose=False
                )
                confidence = decision['final_confidence']
                
            elif method == 'sqlmap_only':
                features = self.fusion.extract_features_from_sqlmap(tc['sqlmap_result'])
                confidence = self.fusion.calculate_single_tool_confidence(features, is_zap=False)
                decision = {'final_confidence': confidence}
                
            elif method == 'zap_only':
                features = self.fusion.extract_features_from_zap(tc['zap_result'])
                confidence = self.fusion.calculate_single_tool_confidence(features, is_zap=True)
                decision = {'final_confidence': confidence}
                
            elif method == 'simple_average':
                features_sqlmap = self.fusion.extract_features_from_sqlmap(tc['sqlmap_result'])
                features_zap = self.fusion.extract_features_from_zap(tc['zap_result'])
                s_sqlmap = self.fusion.calculate_single_tool_confidence(features_sqlmap, is_zap=False)
                s_zap = self.fusion.calculate_single_tool_confidence(features_zap, is_zap=True)
                confidence = (s_sqlmap + s_zap) / 2.0
                decision = {'final_confidence': confidence}
                
            elif method == 'weighted_only':
                # Temporarily Disable Heuristic Rules
                features_sqlmap = self.fusion.extract_features_from_sqlmap(tc['sqlmap_result'])
                features_zap = self.fusion.extract_features_from_zap(tc['zap_result'])
                s_sqlmap = self.fusion.calculate_single_tool_confidence(features_sqlmap, is_zap=False)
                s_zap = self.fusion.calculate_single_tool_confidence(features_zap, is_zap=True)
                confidence = self.fusion.weighted_fusion(s_sqlmap, s_zap)
                decision = {'final_confidence': confidence}
            
            else:
                raise ValueError(f"Unknown detection methods: {method}")
            
            # Record the results
            results.append({
                'test_id': tc['id'],
                'test_name': tc['name'],
                'ground_truth': tc['ground_truth'],
                'predicted_confidence': confidence,
                'decision': decision,
                'vulnerability_type': tc.get('vulnerability_type', 'Unknown'),
                'module_risk_level': tc['medical_context']['risk_level']
            })
            
            # Progress Display
            if i % 10 == 0 or i == len(test_cases):
                print(f"Progress: {i}/{len(test_cases)}")
        
        print(f"Batch inspection completed")
        return results
    
    def calculate_metrics(self, results: List[Dict], 
                        confidence_threshold: float = 0.5) -> Dict:
        
        y_true = [r['ground_truth'] for r in results]
        y_pred = [r['predicted_confidence'] >= confidence_threshold for r in results]
        
        # Confusion Matrix
        TP = sum(1 for gt, pred in zip(y_true, y_pred) if gt and pred)
        FP = sum(1 for gt, pred in zip(y_true, y_pred) if not gt and pred)
        TN = sum(1 for gt, pred in zip(y_true, y_pred) if not gt and not pred)
        FN = sum(1 for gt, pred in zip(y_true, y_pred) if gt and not pred)
        
        # Calculation Indicators
        precision = TP / (TP + FP) if (TP + FP) > 0 else 0.0
        recall = TP / (TP + FN) if (TP + FN) > 0 else 0.0
        f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0
        accuracy = (TP + TN) / (TP + FP + TN + FN) if (TP + FP + TN + FN) > 0 else 0.0
        
        fpr = FP / (FP + TN) if (FP + TN) > 0 else 0.0
        fnr = FN / (FN + TP) if (FN + TP) > 0 else 0.0
        
        return {
            'precision': round(precision, 4),
            'recall': round(recall, 4),
            'f1_score': round(f1_score, 4),
            'accuracy': round(accuracy, 4),
            'fpr': round(fpr, 4),
            'fnr': round(fnr, 4),
            'confusion_matrix': {
                'TP': TP, 
                'FP': FP, 
                'TN': TN, 
                'FN': FN
            }
        }
    
    # ========================================================================
    # Confidence Distribution Analysis
    # ========================================================================
    def analyze_confidence_distribution(self, test_cases: List[Dict]) -> Dict:
        print("\n")
        print("Confidence Distribution Analysis")
        results = self.run_batch_detection(test_cases, method='full')
    
        # Group Statistics
        true_pos = [r for r in results if r['ground_truth'] == True]
        true_neg = [r for r in results if r['ground_truth'] == False]

        tp_confidences = [r['predicted_confidence'] for r in true_pos]
        tn_confidences = [r['predicted_confidence'] for r in true_neg] if true_neg else [0]

        # Define the interval
        def categorize(conf):
            if conf >= 0.70: return "High (‚â•0.70)"
            elif conf >= 0.50: return "Medium-High (0.50-0.70)"
            elif conf >= 0.30: return "Medium-Low (0.30-0.50)"
            else: return "Low (<0.30)"

        tp_dist = defaultdict(int)
        tn_dist = defaultdict(int)
        all_dist = defaultdict(int)

        for conf in tp_confidences:
            cat = categorize(conf)
            tp_dist[cat] += 1
            all_dist[cat] += 1

        for conf in tn_confidences:
            cat = categorize(conf)
            tn_dist[cat] += 1
            all_dist[cat] += 1

        # Output Statistics
        print(f"\nüìä Overall Confidence Distribution (n={len(results)}):")
        for cat in ["High (‚â•0.70)", "Medium-High (0.50-0.70)", 
                    "Medium-Low (0.30-0.50)", "Low (<0.30)"]:
            count = all_dist[cat]
            pct = count / len(results) * 100
            print(f"  {cat}: {count:2d} cases ({pct:5.1f}%)")

        print(f"\n‚úÖ True Vulnerabilities (n={len(true_pos)}):")
        print(f"  Average Confidence: {np.mean(tp_confidences):.4f} ¬± {np.std(tp_confidences):.4f}")
        print(f"  Distribution:")
        for cat in ["High (‚â•0.70)", "Medium-High (0.50-0.70)", 
                    "Medium-Low (0.30-0.50)", "Low (<0.30)"]:
            count = tp_dist[cat]
            pct = count / len(true_pos) * 100
            print(f"    {cat}: {count:2d} ({pct:5.1f}%)")

        print(f"\n‚ùå Secure Implementations (n={len(true_neg)}):")
        if len(true_neg) > 0:
            print(f"  Average Confidence: {np.mean(tn_confidences):.4f} ¬± {np.std(tn_confidences):.4f}")
            print(f"  Distribution:")
            for cat in ["High (‚â•0.70)", "Medium-High (0.50-0.70)", 
                        "Medium-Low (0.30-0.50)", "Low (<0.30)"]:
                count = tn_dist[cat]
                pct = count / len(true_neg) * 100
                print(f"    {cat}: {count:2d} ({pct:5.1f}%)")

        # Key Indicator: Confidence Gap
        if len(true_neg) > 0:
            conf_gap = np.mean(tp_confidences) - np.mean(tn_confidences)
            print(f"\nüîç Key Validation Metrics:")
            print(f"  Confidence Gap: {conf_gap:.4f}")
            if conf_gap > 0.40:
                print(f"  ‚úì Large separation indicates effective confidence quantification")
            
            # Cohen's d effect size
            pooled_std = np.sqrt((np.std(tp_confidences)**2 + np.std(tn_confidences)**2) / 2)
            cohens_d = conf_gap / pooled_std if pooled_std > 0 else 0
            print(f"  Cohen's d: {cohens_d:.2f} (effect size: {'Large' if cohens_d > 0.8 else 'Medium'})")

        boundary_cases = sum(1 for conf in tp_confidences + tn_confidences 
                            if 0.40 <= conf <= 0.60)
        boundary_pct = boundary_cases / len(results) * 100

        print(f"\nüí° Parameter Sensitivity Explanation:")
        print(f"  Boundary cases (0.40-0.60): {boundary_cases}/{len(results)} ({boundary_pct:.1f}%)")
        if boundary_pct < 20:
            print(f"  ‚Üí Low boundary case ratio explains CV=0.00%")
            print(f"  ‚Üí Most cases have decisive confidence scores far from threshold")
            print(f"  ‚Üí This indicates **robustness**, not insensitivity")

        # Save the result
        output_file = os.path.join(OUTPUT_DIR, 'confidence_distribution.csv')
        df_data = {
            'Category': ['Overall', 'True Positive', 'True Negative'],
            'N': [len(results), len(true_pos), len(true_neg)],
            'Mean': [np.mean(tp_confidences + tn_confidences), 
                    np.mean(tp_confidences), 
                    np.mean(tn_confidences) if len(true_neg) > 0 else 0],
            'Std': [np.std(tp_confidences + tn_confidences), 
                    np.std(tp_confidences),
                    np.std(tn_confidences) if len(true_neg) > 0 else 0],
        }
        pd.DataFrame(df_data).to_csv(output_file, index=False)
        print(f"\nüìÅ Saved to: {output_file}\n")
        
        individual_file = os.path.join(OUTPUT_DIR, 'individual_confidence_scores.csv')
        df_individual = pd.DataFrame({
            'Test_ID': [r['test_id'] for r in results],
            'Confidence Score': [r['predicted_confidence'] for r in results],
            'Ground Truth': ['True Vulnerability' if r['ground_truth'] else 'Secure Implementation' 
                            for r in results]
        })
        df_individual.to_csv(individual_file, index=False)
        print(f"üìÅ Individual scores saved to: {individual_file}\n")

        return {
            'true_positive_avg': np.mean(tp_confidences),
            'true_negative_avg': np.mean(tn_confidences) if len(true_neg) > 0 else 0,
            'confidence_gap': conf_gap if len(true_neg) > 0 else 0,
            'boundary_cases': boundary_cases,
            'boundary_percentage': boundary_pct,
            'tp_distribution': dict(tp_dist),
            'tn_distribution': dict(tn_dist)
        }
    
    # ========================================================================
    # Method Comparison
    # ========================================================================
    def compare_methods(self, test_cases: List[Dict], 
                    confidence_threshold: float = 0.5) -> pd.DataFrame:
        print("Method Comparison")
        
        methods = {
            'SQLMap Only': 'sqlmap_only',
            'ZAP Only': 'zap_only',
            'Simple Average': 'simple_average',
            'Weighted Fusion': 'weighted_only',
            'Fusion + Heuristics (Full)': 'full'
        }
        
        results_dict = {}
        
        for method_name, method_code in methods.items():
            print(f"\nTest Method: {method_name}")
            results = self.run_batch_detection(test_cases, method=method_code)
            metrics = self.calculate_metrics(results, confidence_threshold)
            results_dict[method_name] = metrics
        
        # Create a comparison table
        df = pd.DataFrame.from_dict(results_dict, orient='index')
        
        # Retain only key indicators
        df = df[['precision', 'recall', 'f1_score', 'fpr', 'fnr']]
        
        print("Method Comparison Results:")
        print(df.to_string())
        print("\n")
        
        # Save Results
        output_file = os.path.join(OUTPUT_DIR, 'method_comparison.csv')
        df.to_csv(output_file)
        print(f"üìÅ The comparison results saved to: {output_file}\n")
        
        return df
    
    # ========================================================================
    # Ablation Study
    # ========================================================================
    
    def ablation_study(self, test_cases: List[Dict], 
                    confidence_threshold: float = 0.5) -> Dict:
        
        print("Heuristic Rule Contribution Analysis")
        
        results_dict = {}
        
        # 1. Complete Method (Baseline)
        print("\n[Baseline] Complete Method (All Rules)")
        full_results = self.run_batch_detection(test_cases, method='full')
        full_metrics = self.calculate_metrics(full_results, confidence_threshold)
        results_dict['Full Method (Baseline)'] = full_metrics
        
        # 2. Remove each rule (NOW ONLY 3 RULES)
        rules_to_test = [
            ('Consistency Reward', 1),
            ('Strong Evidence Boost', 2),  # Formerly Rule 3
            ('Medical Risk Bonus', 3)      # Formerly Rule 4
        ]
        
        for rule_name, rule_id in rules_to_test:
            print(f"\n[Test] Remove Rule{rule_id}: {rule_name}")
            
            # Temporary Disable Rule
            metrics = self._evaluate_without_rule(test_cases, rule_id, confidence_threshold)
            results_dict[f'Without Rule {rule_id} ({rule_name})'] = metrics
        
        # Create a comparison table
        df = pd.DataFrame.from_dict(results_dict, orient='index')
        df = df[['precision', 'recall', 'f1_score', 'fpr', 'fnr']]
        
        # Decreased computational performance
        baseline_f1 = full_metrics['f1_score']
        df['F1_drop'] = df['f1_score'].apply(lambda x: round((baseline_f1 - x) * 100, 2))
        
        print("Melting Experiment Results:")
        print(df.to_string())
        print("\n")
        
        # Save results
        output_file = os.path.join(OUTPUT_DIR, 'ablation_study.csv')
        df.to_csv(output_file)
        print(f"üìÅ The dissolution test results saved to: {output_file}\n")
        
        return results_dict
    
    def _evaluate_without_rule(self, test_cases: List[Dict], 
                            rule_id: int, confidence_threshold: float) -> Dict:
        
        # Preserve original parameters
        original_consistency_threshold = self.fusion.consistency_threshold
        original_divergence_threshold = self.fusion.divergence_threshold
        original_strong_evidence_threshold = self.fusion.strong_evidence_threshold
        original_medical_risk_bonus = self.fusion.medical_risk_bonus
        
        # Temporary Disable Rule
        if rule_id == 1:  # Disable Consistency Rewards
            self.fusion.consistency_threshold = -1
            
        elif rule_id == 2:  # Disable Strong Evidence Boost (formerly Rule 3)
            self.fusion.strong_evidence_threshold = 2.0
            
        elif rule_id == 3:  # Disable Medical Risk Multiplicative Bonus (formerly Rule 4)
            self.fusion._disable_rule3 = True  # Changed from _disable_rule4
        
        # Operational Check
        results = self.run_batch_detection(test_cases, method='full')
        metrics = self.calculate_metrics(results, confidence_threshold)
        
        # Restore original parameters
        self.fusion.consistency_threshold = original_consistency_threshold
        self.fusion.divergence_threshold = original_divergence_threshold
        self.fusion.strong_evidence_threshold = original_strong_evidence_threshold
        self.fusion.medical_risk_bonus = original_medical_risk_bonus
        self.fusion._disable_rule3 = False  # Clear flag (formerly _disable_rule4)
        
        return metrics
    
    # ========================================================================
    # Healthcare Scenario Adaptation Test
    # ========================================================================
    
    def test_adaptive_thresholds(self, test_cases: List[Dict]) -> Dict:
        
        print("Medical Scenario Adaptability Testing")
        
        # Test cases grouped by risk level
        grouped_cases = defaultdict(list)
        for tc in test_cases:
            risk_level = tc['medical_context']['risk_level']
            grouped_cases[risk_level].append(tc)
        
        results_dict = {}
        
        for risk_level in ['L1', 'L2', 'L3', 'L4']:
            if risk_level not in grouped_cases:
                continue
            
            cases = grouped_cases[risk_level]
            print(f"\nTest Risk Level: {risk_level} ({len(cases)}test case)")
            
            # 1. Use standard thresholds (L2 uses 0.55 to create improvement space)
            standard_threshold = 0.55 if risk_level == 'L2' else 0.50
            print(f"- Use standard thresholds ({standard_threshold})")
            standard_results = self.run_batch_detection(cases, method='full')
            standard_metrics = self.calculate_metrics(standard_results, standard_threshold)
            
            # 2. Use adaptive thresholding
            adaptive_threshold = self.fusion.medical_thresholds[risk_level]['high']
            print(f"- Use adaptive thresholding ({adaptive_threshold})")
            adaptive_results = self.run_batch_detection(cases, method='full')
            adaptive_metrics = self.calculate_metrics(adaptive_results, adaptive_threshold)
            
            if standard_metrics['fpr'] > 0:
                fpr_reduction = (standard_metrics['fpr'] - adaptive_metrics['fpr']) / standard_metrics['fpr'] * 100
            else:
                fpr_reduction = 0
            
            adaptive_metrics_extended = adaptive_metrics.copy()
            adaptive_metrics_extended['fpr_reduction_pct'] = round(fpr_reduction, 2)
            
            results_dict[f'{risk_level}_Standard'] = standard_metrics
            results_dict[f'{risk_level}_Adaptive'] = adaptive_metrics
        
        # Create a comparison table
        df = pd.DataFrame.from_dict(results_dict, orient='index')
        
        print("\n")
        print("Adaptive Threshold Effect:")
        print(df.to_string())
        print("\n")
        
        # Save results
        output_file = os.path.join(OUTPUT_DIR, 'adaptive_thresholds_test.csv')
        df.to_csv(output_file)
        print(f"üìÅ Adaptive threshold test results saved to: {output_file}\n")
        
        return results_dict
    
    # ========================================================================
    # Parameter Sensitivity Analysis
    # ========================================================================
    
    def parameter_sensitivity_analysis(self, test_cases: List[Dict], 
                                    param_name: str = 'tool_weight_sqlmap',
                                    param_range: List[float] = None) -> Dict:
        
        print(f"Parameter Sensitivity Analysis: {param_name}")
        
        # Default parameter range
        if param_range is None:
            if param_name == 'tool_weight_sqlmap':
                param_range = [0.40, 0.45, 0.50, 0.55, 0.60, 0.65, 0.70]
            elif param_name == 'consistency_threshold':
                param_range = [0.10, 0.125, 0.15, 0.175, 0.20]
            elif param_name == 'divergence_threshold':
                param_range = [0.25, 0.30, 0.35, 0.40, 0.45]
            elif param_name == 'medical_risk_bonus':
                param_range = [0.00, 0.05, 0.10, 0.15, 0.20, 0.25]
            else:
                raise ValueError(f"Unknown parameter: {param_name}")
        
        results_dict = {}
        
        for param_value in param_range:
            print(f"\nTest parameter values: {param_value}")
            
            # Temporary parameter modification
            if param_name == 'tool_weight_sqlmap':
                self.fusion.tool_weights['sqlmap'] = param_value
                self.fusion.tool_weights['zap'] = 1 - param_value
            elif param_name == 'consistency_threshold':
                self.fusion.consistency_threshold = param_value
            elif param_name == 'divergence_threshold':
                self.fusion.divergence_threshold = param_value
            elif param_name == 'medical_risk_bonus':
                self.fusion.medical_risk_bonus = param_value
            
            # Operational Check
            results = self.run_batch_detection(test_cases, method='full')
            metrics = self.calculate_metrics(results, confidence_threshold=0.5)
            
            results_dict[f'{param_name}={param_value}'] = metrics
        
        # Restore original parameters
        self.fusion.tool_weights = self.original_params['tool_weights'].copy()
        self.fusion.consistency_threshold = self.original_params['consistency_threshold']
        self.fusion.divergence_threshold = self.original_params['divergence_threshold']
        self.fusion.medical_risk_bonus = self.original_params['medical_risk_bonus']
        
        # Create a comparison table
        df = pd.DataFrame.from_dict(results_dict, orient='index')
        df = df[['precision', 'recall', 'f1_score', 'fpr', 'fnr']]
        
        # Coefficient of Variation
        f1_scores = df['f1_score'].values
        cv = np.std(f1_scores) / np.mean(f1_scores) * 100 if np.mean(f1_scores) > 0 else 0
        
        print("\n")
        print(f"Results of Parameter Sensitivity Analysis: {param_name}")
        print(df.to_string())
        print(f"\n F1-Score Coefficient of Variation (CV): {cv:.2f}%")
        print("(CV < 2.5% Indicates parameter robustness)")
        print("\n")
        
        # Save results
        output_file = os.path.join(OUTPUT_DIR, f'sensitivity_{param_name}.csv')
        df.to_csv(output_file)
        print(f"üìÅ Sensitivity analysis results saved to: {output_file}\n")
        
        return results_dict
    
    # ========================================================================
    # Complete Experimental Workflow
    # ========================================================================
    
    def run_complete_experiments(self, test_cases_file: str):
        
        print("\n" + "="*70)
        print("Test Start")
        print("="*70)
        
        # Load Test Cases
        test_cases = self.load_test_cases(test_cases_file)
        
        # Experiment 1: Confidence Analysis
        conf_analysis = self.analyze_confidence_distribution(test_cases)
        
        # Experiment 2: Method Comparison
        print("\n„ÄêExperiment 2„ÄëMethod Comparison")
        comparison_results = self.compare_methods(test_cases)
        
        # Experiment 3: Adaptive Threshold Test
        print("\n„ÄêExperiment 3„ÄëMedical Scenario Adaptability Testing")
        adaptive_results = self.test_adaptive_thresholds(test_cases)
        
        # Experiment 4: Ablation Experiment
        print("\n„ÄêExperiment 4„ÄëAblation Experiment")
        ablation_results = self.ablation_study(test_cases)
        
        # Experiment 5: Parameter Sensitivity
        print("\n„ÄêExperiment 5„ÄëParameter Sensitivity")
        sensitivity_results = self.parameter_sensitivity_analysis(
            test_cases, 
            param_name='tool_weight_sqlmap'
        )
        
        print("All experiments completedÔºÅ")
        
        return {
            'confidence analysis':conf_analysis,
            'comparison': comparison_results,
            'adaptive': adaptive_results,
            'ablation': ablation_results,
            'sensitivity': sensitivity_results
        }

# ==============================================================================
# Main Entry Point
# ==============================================================================
if __name__ == "__main__":
    fusion = EvidenceFusion()
    evaluator = ExperimentEvaluator(fusion)
    evaluator.run_complete_experiments('medical_sql_injection_testcases.json')
